{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. Deliver data from https://docs.google.com/document/d/1VTisnIBafttzCNPAlEV149Mhyqc7D2Q_96OA9hKmp_M/edit\n",
    "2. Prototype using wmf replica databases with jupyter on local through ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from pymysql.err import InternalError, OperationalError\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mwclient\n",
    "import mwviews\n",
    "import mwapi\n",
    "import mwreverts\n",
    "import mwreverts.api\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "import time\n",
    "import functools\n",
    "\n",
    "def wmftimestamp(bytestring):\n",
    "    if bytestring:\n",
    "        s = bytestring.decode('utf-8')\n",
    "        return dt.strptime(s, '%Y%m%d%H%M%S')\n",
    "    else:\n",
    "        return bytestring\n",
    "    \n",
    "def decode_or_none(b):\n",
    "    return b.decode('utf-8') if b else None\n",
    "\n",
    "sim_treatment_dates = (dt(2018,3,6), dt(2018,3, 23))\n",
    "sim_treatment_date = sim_treatment_dates[0]\n",
    "\n",
    "sim_experiment_end_date = sim_treatment_date + td(days=90)\n",
    "sim_observation_start_date = sim_treatment_date - td(days=90)\n",
    "\n",
    "GRAT_DIR = '/home/paprika/workspace/civilservant_wiki_gratitude/data'\n",
    "\n",
    "def timeit(func):\n",
    "    @functools.wraps(func)\n",
    "    def newfunc(*args, **kwargs):\n",
    "        startTime = time.time()\n",
    "        func(*args, **kwargs)\n",
    "        elapsedTime = time.time() - startTime\n",
    "        print('function [{}] finished in {} ms'.format(\n",
    "            func.__name__, int(elapsedTime * 1000)))\n",
    "    return newfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f8df2d39630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constr = 'mysql+pymysql://{user}:{pwd}@{host}:{port}'.format(user=os.environ['MYSQL_USERNAME'],\n",
    "                                                      pwd=os.environ['MYSQL_PASSWORD'],\n",
    "                                                      host=os.environ['MYSQL_HOST'],\n",
    "                                                    port=os.environ['MYSQL_PORT'],\n",
    "                                                         charset='utf8',\n",
    "                                                        use_unicode=True)\n",
    "\n",
    "con = create_engine(constr, encoding='utf-8')\n",
    "\n",
    "\n",
    "con.execute(f'use enwiki_p;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POPULATIONS\n",
    "\n",
    "# ar_is_autoreviewer (binary): does this account have autoreviewer status? If the account isn't from that language, the value should be NA\n",
    "# de_is_autoreviewer (binary): does this account have flagged revisions permission on DE Wikipedia?  If the account isn't from that language, the value should be NA\n",
    "# de_is_days_enough (binary): has this user been active for 60 days\n",
    "# de_is_edits_enough (binary): has the user more than 300 edits\n",
    "# pl_is_editor (binary): does this account have flagged revisions permission on PL Wikipedia?  If the account isn't from that language, the value should be NA\n",
    "# fa_is_days_enough (binary): is 365 days or more registered\n",
    "# fa_is_edits_enough (binary): is 500 edits or more \n",
    "DE_EDITS_ENOUGH = 300\n",
    "DE_DAYS_ENOUGH = 60\n",
    "FA_EDITS_ENOUGH = 500\n",
    "FA_DAYS_ENOUGH = 365 \n",
    "\n",
    "de_pop_sql = \"\"\"select user_id, ug_group, user_name, user_editcount, user_registration from (\n",
    "    select user_id, ug_group, user_name, user_editcount, coalesce(user_registration, 20010101000000) as user_registration\n",
    "          from (select * from user_groups where ug_group = 'autoreview') ug\n",
    "join user u on  ug.ug_user = u.user_id) coal\n",
    "where user_editcount >= {DE_EDITS_ENOUGH} and user_registration <= {reg_start};\n",
    "    \"\"\".format(reg_start=(sim_treatment_date-td(days=DE_DAYS_ENOUGH)).strftime('%Y%m%d%H%M%S'), \n",
    "               DE_EDITS_ENOUGH=DE_EDITS_ENOUGH)\n",
    "\n",
    "fa_pop_sql = \"\"\"select * from (\n",
    "    select user_id, user_name, user_editcount, coalesce(user_registration, 20010101000000) as user_registration from user) u\n",
    "                    where user_editcount >= {FA_EDITS_ENOUGH} and user_registration <= {reg_start};\n",
    "    \"\"\".format(reg_start=(sim_treatment_date-td(days=FA_DAYS_ENOUGH)).strftime('%Y%m%d%H%M%S'),\n",
    "              FA_EDITS_ENOUGH=FA_EDITS_ENOUGH)\n",
    "                    \n",
    "\n",
    "def user_group_members(user_group):\n",
    "    return \"\"\"select * from (\n",
    "select user_id, user_name, ug_group, coalesce(user_registration, 20010101000000) as user_registration \n",
    "from (select * from user_groups where ug_group = '{user_group}') ug\n",
    "  join user u on ug.ug_user = u.user_id) coalesced\n",
    "  where user_registration <= {reg_start};\"\"\".format(reg_start=sim_treatment_date.strftime('%Y%m%d%H%M%S'),\n",
    "                                                   user_group=user_group)\n",
    "\n",
    "lang_sqlparams = {'de': {'pop_sql': de_pop_sql, 'true_cols_to_add': ['de_is_days_enough', 'de_is_edits_enough', 'de_is_autoreviewer']},\n",
    "                  'ar': {'pop_sql': user_group_members('autoreview'), 'true_cols_to_add': ['ar_is_autoreview']},\n",
    "                  'pl': {'pop_sql': user_group_members('editor'), 'true_cols_to_add': ['pl_is_editor']},\n",
    "                  'fa': {'pop_sql': fa_pop_sql, 'true_cols_to_add': ['fa_is_days_enough', 'fa_is_edits_enough']}\n",
    "                }\n",
    "\n",
    "def create_thanker_pop(lang, pop_sql, true_cols_to_add):\n",
    "    cache_key = f'cache/pops/{lang}'\n",
    "    if os.path.exists(cache_key):\n",
    "        return pd.read_pickle(cache_key)\n",
    "    else:\n",
    "        con.execute(f'use {lang}wiki_p;')\n",
    "        df = pd.read_sql(pop_sql, con)\n",
    "        decode_cols = ['ug_group', 'user_name',]\n",
    "        timestamp_cols = ['user_registration']\n",
    "        for decode_col in decode_cols:\n",
    "            try:\n",
    "                df[decode_col] = df[decode_col].apply(decode_or_none)\n",
    "            except KeyError:\n",
    "                df[decode_col] = float('nan')\n",
    "        for timestamp_col in timestamp_cols:\n",
    "            df[timestamp_col] = df[timestamp_col].apply(wmftimestamp)\n",
    "        for true_col_to_add in true_cols_to_add:\n",
    "            df[true_col_to_add] = True\n",
    "\n",
    "        df['lang'] = lang\n",
    "        df.to_pickle(cache_key)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCKS (aka. Bans)\n",
    "def get_bans(lang, start_date, end_date):\n",
    "    cache_key = f'cache/bans/{lang}_{start_date}_{end_date}.pickle'\n",
    "    if not os.path.exists(cache_key):\n",
    "        start_stamp = start_date.strftime('%Y%m%d%H%M%S')\n",
    "        end_stamp = end_date.strftime('%Y%m%d%H%M%S')\n",
    "        con.execute(f'use {lang}wiki_p;')\n",
    "        ban_sql = f\"\"\"select log_user as blocking_user_id, log_user_text as blocking_user_name, log_title as blocked_user_name \n",
    "        from logging where log_action='block' \n",
    "        and log_timestamp >= {start_stamp} and log_timestamp < {end_stamp};\"\"\"\n",
    "#         print(ban_sql)\n",
    "        ban_df = pd.read_sql(ban_sql, con)\n",
    "        ban_df['blocking_user_name'] = ban_df['blocking_user_name'].apply(decode_or_none)\n",
    "        ban_df['blocked_user_name'] = ban_df['blocked_user_name'].apply(decode_or_none)\n",
    "        ban_df['lang'] = lang\n",
    "        band_df = ban_df[pd.notnull(ban_df['blocking_user_id'])]\n",
    "        ban_df.to_pickle(cache_key)\n",
    "    else:\n",
    "        ban_df = pd.read_pickle(cache_key)\n",
    "    \n",
    "    return ban_df\n",
    "\n",
    "def add_blocks(start_date, end_date, col_label, df):\n",
    "    ban_dfs = []\n",
    "    for lang in lang_sqlparams.keys():\n",
    "    #     print(lang)\n",
    "        t0 = time.time()\n",
    "        ban_df = get_bans(lang, start_date, end_date)\n",
    "        ban_dfs.append(ban_df)\n",
    "        print(f'{lang} took {time.time()-t0}.')\n",
    "\n",
    "    bans = pd.concat(ban_dfs)\n",
    "\n",
    "\n",
    "    user_ban_counts = pd.DataFrame(bans.groupby(['lang','blocking_user_id']).size())\n",
    "    print(user_ban_counts.head())\n",
    "\n",
    "    df = pd.merge(df, user_ban_counts, left_on=['lang', 'user_id'], right_on=['lang', 'blocking_user_id'], how='left').rename(columns={0:col_label})\n",
    "    return df\n",
    "\n",
    "#@timeit\n",
    "def add_blocks_pre_treatment(df):\n",
    "    return add_blocks(sim_observation_start_date, sim_treatment_date, \"block_actions_90_pre_treatment\", df)\n",
    "\n",
    "#@timeit\n",
    "def add_blocks_post_treatment(df):\n",
    "    return add_blocks(sim_treatment_date, sim_experiment_end_date, \"block_actions_90_post_treatment\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just cache user histories\n",
    "def get_user_edits(lang, user_id, start_date, end_date):\n",
    "    cache_key = f'cache/edithistory/{lang}_{user_id}_{start_date}_{end_date}.pickle'\n",
    "    if not os.path.exists(cache_key):\n",
    "        start_stamp = start_date.strftime('%Y%m%d%H%M%S')\n",
    "        end_stamp = end_date.strftime('%Y%m%d%H%M%S')\n",
    "        con.execute(f'use {lang}wiki_p;')\n",
    "        user_sql = f\"\"\"select rev_id, rev_timestamp, page_id, page_namespace from\n",
    "                        (select * from revision_userindex \n",
    "                         where rev_user = {user_id} and\n",
    "                          rev_timestamp >= {start_stamp} and rev_timestamp < {end_stamp})\n",
    "                        user_revs\n",
    "                        join page where rev_page = page_id;\"\"\"\n",
    "\n",
    "        user_df = pd.read_sql(user_sql, con)\n",
    "        user_df['rev_timestamp'] = user_df['rev_timestamp'].apply(wmftimestamp)\n",
    "        user_df['lang'] = lang\n",
    "\n",
    "        user_df.to_pickle(cache_key)\n",
    "    else:\n",
    "        user_df = pd.read_pickle(cache_key)\n",
    "    \n",
    "    return user_df\n",
    "\n",
    "#@timeit\n",
    "def cache_all_user_edits(df):\n",
    "    count = 0\n",
    "    for lang in lang_sqlparams.keys():\n",
    "        for start_date, end_date in ((sim_observation_start_date, sim_treatment_date), \n",
    "                                     (sim_treatment_date, sim_experiment_end_date)):\n",
    "            user_ids = df[df['lang']==lang]['user_id'].values\n",
    "            for user_id in user_ids:\n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'Count {count} lang {lang} userid {user_id}')\n",
    "                user_df = get_user_edits(lang, user_id, start_date, end_date)\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_revs = get_num_reverts('fa', user_id=630305, user_df=user_df, start_date=sim_observation_start_date, end_date=sim_treatment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVERTS\n",
    "def get_num_reverts(lang, user_id, user_df, start_date, end_date):\n",
    "    cache_key = f'cache/reverts/{lang}_{user_id}_{start_date}_{end_date}.pickle'\n",
    "    if os.path.exists(cache_key):\n",
    "        return pd.read_pickle(cache_key)\n",
    "    else:\n",
    "        session = mwapi.Session(f\"https://{lang}.wikipedia.org\", user_agent=\"max.klein@civilservant.io gratitude power analysis generator\")\n",
    "        revertings = 0\n",
    "        for rev_id in user_df['rev_id'].values:\n",
    "            try:\n",
    "                reverting, reverted, reverted_to = mwreverts.api.check(session, rev_id)\n",
    "                if reverting:\n",
    "                    revertings += 1\n",
    "            except mwapi.session.APIError:\n",
    "                continue\n",
    "        col_name_suffix = 'pre' if start_date < sim_treatment_date else 'post'\n",
    "        col_name = f'num_reverts_90_{col_name_suffix}_treatment'\n",
    "        user_reverts_df = pd.DataFrame.from_dict({col_name:[revertings],'user_id':[user_id], 'lang':[lang]}, orient='columns')\n",
    "        user_reverts_df.to_pickle(cache_key)\n",
    "        return user_reverts_df\n",
    "\n",
    "def create_reverts_df(df, start_date, end_date):\n",
    "    count = 0\n",
    "    reverts_dfs = []\n",
    "    for lang in lang_sqlparams.keys():\n",
    "            user_ids = df[df['lang']==lang]['user_id'].values\n",
    "            for user_id in user_ids:\n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'Count {count} lang {lang} userid {user_id}')\n",
    "                user_df = get_user_edits(lang, user_id, start_date, end_date)\n",
    "                t0 = time.time()\n",
    "                user_revert_df = get_num_reverts(lang, user_id, user_df, start_date, end_date)\n",
    "#                 print(f'user_id {user_id} took {time.time()-t0} to get reverts')\n",
    "                reverts_dfs.append(user_revert_df)\n",
    "                count += 1\n",
    "    reverts_df = pd.concat(reverts_dfs)\n",
    "    return reverts_df\n",
    "\n",
    "def create_and_merge_revert_actions(df, start_date, end_date):\n",
    "    reverts_df = create_reverts_df(df, start_date, end_date)\n",
    "    df = pd.merge(df, reverts_df, how='left', on=['lang', 'user_id'])\n",
    "    return df\n",
    "\n",
    "#@timeit\n",
    "def add_revert_actions_pre_treatment(df):\n",
    "    return create_and_merge_revert_actions(df, start_date=sim_observation_start_date, end_date=sim_treatment_date)\n",
    "\n",
    "#@timeit\n",
    "def add_revert_actions_post_treatment(df):\n",
    "    return create_and_merge_revert_actions(df, start_date=sim_treatment_date, end_date=sim_experiment_end_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df = get_user_edits(lang='fa', user_id=630305, start_date=sim_observation_start_date, end_date=sim_treatment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df['page_namespace'].apply(is_talk_page).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TALKPAGES\n",
    "def get_talk_counts(lang, user_id, user_df, start_date, end_date, namespace_fn):\n",
    "    talk_count = user_df['page_namespace'].apply(namespace_fn['fn']).sum()\n",
    "    user_talk_df = pd.DataFrame.from_dict({namespace_fn['col']:[talk_count],\n",
    "                                           'user_id':[user_id], \n",
    "                                           'lang':[lang]}, orient='columns')\n",
    "    return user_talk_df\n",
    "\n",
    "def create_talk_df(df, start_date, end_date, namespace_fn):\n",
    "    count = 0\n",
    "    talk_dfs = []\n",
    "    for lang in lang_sqlparams.keys():\n",
    "            user_ids = df[df['lang']==lang]['user_id'].values\n",
    "            for user_id in user_ids:\n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'Count {count} lang {lang} userid {user_id}')\n",
    "                user_df = get_user_edits(lang, user_id, start_date, end_date)\n",
    "                t0 = time.time()\n",
    "                user_talk_df = get_talk_counts(lang, user_id, user_df, start_date, end_date, namespace_fn)\n",
    "#                 print(f'user_id {user_id} took {time.time()-t0} to get reverts')\n",
    "                talk_dfs.append(user_talk_df)\n",
    "    talk_df = pd.concat(talk_dfs)\n",
    "    return talk_df\n",
    "\n",
    "\n",
    "def create_and_merge_talk(df, start_date, end_date, namespace_fn):\n",
    "    talk_df = create_talk_df(df, start_date, end_date, namespace_fn)\n",
    "    df = pd.merge(df, talk_df, how='left', on=['lang', 'user_id'])\n",
    "    return df\n",
    "\n",
    "def is_wp_page(namespace):\n",
    "    return namespace in [4,5] # maybe in (4,5)\n",
    "\n",
    "def is_talk_page(namespace):\n",
    "    return namespace % 2 == 1\n",
    "\n",
    "#@timeit\n",
    "def add_support_talk_90_pre_treatment(df):\n",
    "    return create_and_merge_talk(df, start_date=sim_observation_start_date, end_date=sim_treatment_date,\n",
    "                                namespace_fn={'col':'support_talk_90_pre_treatment', 'fn': is_talk_page})\n",
    "\n",
    "#@timeit\n",
    "def add_support_talk_90_post_treatment(df):\n",
    "    return create_and_merge_talk(df, start_date=sim_treatment_date, end_date=sim_experiment_end_date,\n",
    "                                namespace_fn={'col':'support_talk_90_post_treatment', 'fn': is_talk_page})\n",
    "\n",
    "#@timeit\n",
    "def add_project_talk_90_pre_treatment(df):\n",
    "    return create_and_merge_talk(df, start_date=sim_observation_start_date, end_date=sim_treatment_date,\n",
    "                                namespace_fn={'col':'project_talk_90_pre_treatment', 'fn': is_wp_page})\n",
    "\n",
    "#@timeit\n",
    "def add_project_talk_90_post_treatment(df):\n",
    "    return create_and_merge_talk(df, start_date=sim_treatment_date, end_date=sim_experiment_end_date,\n",
    "                                namespace_fn={'col':'project_talk_90_post_treatment', 'fn': is_wp_page})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCOURAGEMENT\n",
    "\n",
    "class preloaded_csvs():\n",
    "    def __init__(self):\n",
    "        self.path_dfs = {}\n",
    "        \n",
    "    def get_csv(self, csv_path):\n",
    "        try:\n",
    "            return self.path_dfs[csv_path]\n",
    "        except KeyError:\n",
    "            grat_df = pd.read_csv(csv_path, usecols=['timestamp', 'sender_id'], parse_dates=[0])\n",
    "            self.path_dfs[csv_path] = grat_df\n",
    "            return grat_df\n",
    "        \n",
    "\n",
    "def get_num_grats(lang, user_id, user_df, start_date, end_date, grat_type, preloaded):\n",
    "    cache_key = f'cache/{grat_type}/{lang}_{user_id}_{start_date}_{end_date}.pickle'\n",
    "    if os.path.exists(cache_key):\n",
    "        return pd.read_pickle(cache_key)\n",
    "    else:\n",
    "        # this could be optimized by keeping this in memory\n",
    "        csv_path = os.path.join(GRAT_DIR, lang, 'outputs')\n",
    "        lsdir = os.listdir(csv_path)\n",
    "        try:\n",
    "            grat_csv = [f for f in lsdir if grat_type in f][0]\n",
    "            grat_csv_path = os.path.join(csv_path, grat_csv)\n",
    "            grat_df = preloaded.get_csv(grat_csv_path)\n",
    "            user_grats = grat_df[grat_df['sender_id']==user_id]\n",
    "            user_grats = user_grats[(user_grats['timestamp']<end_date) & (user_grats['timestamp']>=start_date)]\n",
    "            num_grats = len(user_grats)\n",
    "        except IndexError: #occurs when language doesnt have wikilove\n",
    "            num_grats = float('nan')\n",
    "        col_name_suffix = 'pre' if start_date < sim_treatment_date else 'post'\n",
    "        col_name = f'wiki{grat_type}_90_{col_name_suffix}_treatment'\n",
    "        user_grat_df = pd.DataFrame.from_dict({col_name:[num_grats],\n",
    "                                           'user_id':[user_id], \n",
    "                                           'lang':[lang]}, orient='columns')\n",
    "        return user_grat_df\n",
    "\n",
    "def create_grat_df(df, start_date, end_date, grat_type):\n",
    "    count = 0\n",
    "    grat_dfs = []\n",
    "    preloaded = preloaded_csvs()\n",
    "    for lang in lang_sqlparams.keys():\n",
    "            user_ids = df[df['lang']==lang]['user_id'].values\n",
    "            for user_id in user_ids:\n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'Count {count} lang {lang} userid {user_id}')\n",
    "                user_df = get_user_edits(lang, user_id, start_date, end_date)\n",
    "                t0 = time.time()\n",
    "                user_grat_df = get_num_grats(lang, user_id, user_df, start_date, end_date, grat_type, preloaded)\n",
    "#                 print(f'user_id {user_id} took {time.time()-t0} to get reverts')\n",
    "                grat_dfs.append(user_grat_df)\n",
    "                count += 1\n",
    "    grat_df = pd.concat(grat_dfs)\n",
    "    return grat_df\n",
    "\n",
    "def create_and_merge_encouragement(df, start_date, end_date, grat_type):\n",
    "    grat_df = create_grat_df(df, start_date, end_date, grat_type)\n",
    "    df = pd.merge(df, grat_df, how='left', on=['lang', 'user_id'])\n",
    "    return df\n",
    "\n",
    "#@timeit\n",
    "def add_thanks_90_pre_treatment(df):\n",
    "    return create_and_merge_encouragement(df, start_date=sim_observation_start_date, end_date=sim_treatment_date,\n",
    "                                         grat_type='thank')\n",
    "\n",
    "#@timeit\n",
    "def add_thanks_90_post_treatment(df):\n",
    "    return create_and_merge_encouragement(df, start_date=sim_treatment_date, end_date=sim_experiment_end_date,\n",
    "                                         grat_type='thank')\n",
    "\n",
    "#@timeit\n",
    "def add_wikilove_90_pre_treatment(df):\n",
    "    return create_and_merge_encouragement(df, start_date=sim_observation_start_date, end_date=sim_treatment_date,\n",
    "                                         grat_type='love')\n",
    "\n",
    "#@timeit\n",
    "def add_wikilove_90_post_treatment(df):\n",
    "    return create_and_merge_encouragement(df, start_date=sim_treatment_date, end_date=sim_experiment_end_date,\n",
    "                                         grat_type='love')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@timeit\n",
    "def get_populations():\n",
    "    lang_dfs = []\n",
    "\n",
    "    for lang, sqlparams in lang_sqlparams.items():\n",
    "        # print(f'Doing {lang}, with {sqlparams}.')\n",
    "        lang_df = create_thanker_pop(lang, **sqlparams)\n",
    "        lang_dfs.append(lang_df)\n",
    "\n",
    "    df = pd.concat(lang_dfs)\n",
    "    del lang_dfs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to make data\n",
      "making populations\n",
      "subsetting to 100 samples\n",
      "adding blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paprika/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de took 290.29703760147095.\n"
     ]
    }
   ],
   "source": [
    "  def make_data(subsample=None):\n",
    "    print('starting to make data')\n",
    "    print('making populations')\n",
    "    df = get_populations()\n",
    "    if subsample:\n",
    "        print(f'subsetting to {subsample} samples')\n",
    "        df = df.sample(n=subsample, random_state=1854)\n",
    "    \n",
    "    print('adding blocks')\n",
    "    df = add_blocks_pre_treatment(df)\n",
    "    df = add_blocks_post_treatment(df)\n",
    "    # get user edits:\n",
    "    cache_all_user_edits(df)\n",
    "    \n",
    "    print('adding reverts')\n",
    "    df = add_revert_actions_pre_treatment(df)\n",
    "    df = add_revert_actions_post_treatment(df)\n",
    "    \n",
    "#     df = add_protective_actions_pre_treament(df)\n",
    "#     df = add_protective_actions_post_treament(df)\n",
    "    print('adding support talk')\n",
    "    df = add_support_talk_90_pre_treatment(df)\n",
    "    df = add_support_talk_90_post_treatment(df)\n",
    "        \n",
    "    print('adding project talk')\n",
    "    df = add_project_talk_90_pre_treatment(df)\n",
    "    df = add_project_talk_90_post_treatment(df)\n",
    "\n",
    "    print('adding wikithanks')\n",
    "    df = add_thanks_90_pre_treatment(df)\n",
    "    df = add_thanks_90_post_treatment(df)\n",
    "    \n",
    "    \n",
    "    print('adding wikiloves')\n",
    "    df = add_wikilove_90_pre_treatment(df)\n",
    "    df = add_wikilove_90_post_treatment(df)\n",
    "    \n",
    "    print('finished making data')\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     import subprocess\n",
    "#     subprocess.Popen('scripts/wmf_ssh_tunnel.sh')\n",
    "    conf = json.load(open('config/default.json','r'))\n",
    "    subsample = conf['subsample'] if 'subsample' in conf.keys() else None\n",
    "    df = make_data(conf['subsample'])\n",
    "    df.to_csv(f'outputs/thanker_power_analysis_data_for_sim_treatment_{sim_treatment_date.strftime(\"%Y%m%d\")}{(\"_\"+str(subsample)+\"_subsamples\") if subsample else \"\"}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
