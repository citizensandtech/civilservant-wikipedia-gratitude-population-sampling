{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "1. functional refactor\n",
    "1. plus num_edits_all_time_pre_treatment\n",
    "plus has_email_configured (as proxy for emailable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def window_seq(seq, n=2):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "        \n",
    "def wmftimestamp(bytestring):\n",
    "    if bytestring:\n",
    "        s = bytestring.decode('utf-8')\n",
    "        return dt.strptime(s, '%Y%m%d%H%M%S')\n",
    "    else:\n",
    "        return bytestring\n",
    "    \n",
    "def to_wmftimestamp(date):\n",
    "    return date.strftime('%Y%m%d%H%M%S')\n",
    "    \n",
    "    \n",
    "\n",
    "def decode_or_nan(b):\n",
    "    return b.decode('utf-8') if b else float('nan')\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from pymysql.err import InternalError, OperationalError\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mwclient\n",
    "import mwviews\n",
    "import mwapi\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "langs_non_de = ['ar', 'fa', 'pl']\n",
    "langs = langs_non_de + ['de']\n",
    "\n",
    "# site = mwclient.Site(('https', f'{langcode}.wikipedia.org'), path = '/w/')\n",
    "\n",
    "# os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "constr = 'mysql+pymysql://{user}:{pwd}@{host}:{port}'.format(user=os.environ['MYSQL_USERNAME'],\n",
    "                                                      pwd=os.environ['MYSQL_PASSWORD'],\n",
    "                                                      host=os.environ['MYSQL_HOST'],\n",
    "                                                    port=os.environ['MYSQL_PORT'],\n",
    "                                                         charset='utf8',\n",
    "                                                        use_unicode=True)\n",
    "\n",
    "con = create_engine(constr, encoding='utf-8')\n",
    "\n",
    "\n",
    "con.execute(f'use enwiki_p;')\n",
    "\n",
    "#Tuesday Mar 6 2018\n",
    "#Friday March 23 2018\n",
    "\n",
    "sim_treatment_dates = (dt(2018,3,6), dt(2018,3, 23))\n",
    "sim_treatment_date = sim_treatment_dates[0]\n",
    "\n",
    "sim_experiment_end_date = sim_treatment_date + td(days=90)\n",
    "sim_observation_start_date = sim_treatment_date - td(days=90)\n",
    "\n",
    "# wikipedia_start_date = dt(2001,1,1)\n",
    "wikipedia_start_date = dt(2015,1,1)\n",
    "\n",
    "CACHE_ROOT = os.getenv('CACHE_DIR', './cache')\n",
    "GRAT_ROOT = os.getenv('GRAT_DIR', '../gratitude/outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwreverts.api\n",
    "import mwapi\n",
    "\n",
    "# We'll use the mwreverts API check.  In order to do that, we need an API session\n",
    "de_session = mwapi.Session(\"https://de.wikipedia.org\", \n",
    "                        user_agent=\"Max Klein Newcomer Quality <max@notconfusing.com>\")\n",
    "\n",
    "# for a single user, get their revisions and determine how many are flagged or not\n",
    "\n",
    "def get_revisions_and_flagged_data(user_id, treatment_date):\n",
    "    rev_flag_sql = \"\"\"\n",
    "        select rev_id, \n",
    "                rev_page, \n",
    "                page_namespace, \n",
    "                rev_timestamp, \n",
    "                fr_timestamp, \n",
    "                (select max(fr_timestamp) from flaggedrevs where fr_page_id=rev_page and fr_timestamp < {treatment_date}) max_fr_ts \n",
    "            from (\n",
    "                  select rev_id, rev_page, rev_timestamp, page_namespace from revision_userindex\n",
    "                    join page on page_id = rev_page where rev_user={user_id} \n",
    "                    and rev_timestamp < {treatment_date}\n",
    "                    order by rev_timestamp desc limit 500) auser\n",
    "            left join flaggedrevs on\n",
    "                fr_page_id = rev_page and\n",
    "                fr_rev_id = rev_id;\n",
    "                    \"\"\".format(user_id=user_id, treatment_date=treatment_date.strftime('%Y%m%d%H%M%S'))\n",
    "    con.execute('use dewiki_p;')\n",
    "    rev_flag = pd.read_sql(rev_flag_sql, con)\n",
    "    rev_flag['fr_timestamp'] = rev_flag['fr_timestamp'].apply(wmftimestamp)\n",
    "    rev_flag['max_fr_ts'] = rev_flag['max_fr_ts'].apply(wmftimestamp)\n",
    "    rev_flag['rev_timestamp'] = rev_flag['rev_timestamp'].apply(wmftimestamp)\n",
    "    return rev_flag\n",
    "\n",
    "def was_reverted(rev_id):\n",
    "    try:\n",
    "        _, reverted, reverted_to = mwreverts.api.check(\n",
    "            de_session, rev_id, radius=3,  # most reverts within 5 edits\n",
    "            window=48*60*60,  # 2 days\n",
    "            rvprop={'user', 'ids'})  # Some properties we'll make use of\n",
    "        return True if reverted else False\n",
    "    except (KeyError, mwapi.session.APIError) as err:\n",
    "        print('Error getting revert status for {rev_id}'.format(rev_id=rev_id))\n",
    "        return True #because even if it was deleted from the DB for our purposes its still a bad edit \n",
    "\n",
    "def decide_flagged(row):\n",
    "    \"\"\"Was this revision flagged (or generally high quality)?\"\"\"\n",
    "    namespace = row['page_namespace']\n",
    "    rev_time = row['rev_timestamp']\n",
    "    flagged_time = row['fr_timestamp']\n",
    "    last_flagged_time = row['max_fr_ts']\n",
    "    was_reverted = row['was_reverted']\n",
    "    \n",
    "    #namespace check\n",
    "    if namespace != 0:\n",
    "        return True # because often user-page edits are never approved   \n",
    "    # check if explictly flagged\n",
    "    elif pd.notnull(flagged_time):\n",
    "        return True\n",
    "    #check if reverted\n",
    "    elif was_reverted:\n",
    "        return False\n",
    "    #check if the last flagged time is after the edit\n",
    "    elif last_flagged_time:\n",
    "        if rev_time < last_flagged_time:\n",
    "            return True\n",
    "        #the revision exists but it hasn't been flagged yet.\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        #not sure what else would get to this stage , but...\n",
    "        return False\n",
    "        \n",
    "def get_flagged_decision_df(user_id, treatment_date):\n",
    "    rev_df = get_revisions_and_flagged_data(user_id, treatment_date)\n",
    "    rev_df['was_reverted'] = rev_df.apply(lambda row: was_reverted(row['rev_id']) if pd.isnull(row['fr_timestamp']) else 'no_check', axis=1)\n",
    "    rev_df['flagged'] = rev_df.apply(decide_flagged, axis=1)\n",
    "    return rev_df\n",
    "\n",
    "\n",
    "def get_flagged_revs(user_id, treatment_date, return_all=False):\n",
    "    needed_columns = ['user_id', 'rev_id', 'rev_timestamp','was_flagged', 'was_reverted']\n",
    "    rev_df = get_flagged_decision_df(user_id, treatment_date)\n",
    "    rev_df['user_id'] = user_id\n",
    "    rev_df['was_flagged'] = rev_df.apply(decide_flagged, axis=1)\n",
    "    if return_all:\n",
    "        return rev_df\n",
    "    #limit to only flagged\n",
    "    rev_df = rev_df[rev_df['was_flagged']==True]\n",
    "    # limit ot non-talk pages\n",
    "    rev_df = rev_df[rev_df['page_namespace'].apply(lambda ns: ns%2==0)]\n",
    "    # Limit to 17\n",
    "    if rev_df.empty:\n",
    "        return pd.DataFrame(columns=needed_columns)\n",
    "    rev_df = rev_df.sort_values('rev_timestamp', ascending=False)\n",
    "    rev_df = rev_df.iloc[:17]\n",
    "\n",
    "    return rev_df[needed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cached_df(cache_sub_dir):\n",
    "    #decorator factory\n",
    "    def cached_df(df_returning_fn):\n",
    "        #decorator\n",
    "        def get_with_cache(*args, **kwargs):\n",
    "            #wrapping function\n",
    "            cache_dir = os.path.join(CACHE_ROOT, cache_sub_dir)\n",
    "            if not os.path.exists(cache_dir):\n",
    "                    os.makedirs(cache_dir, exist_ok=True)\n",
    "            fname = f'{\"_\".join([str(a) for a in args])}'\n",
    "            cache_key = os.path.join(cache_dir, fname)\n",
    "            try:\n",
    "                return pd.read_pickle(cache_key)\n",
    "            except FileNotFoundError:\n",
    "                df = df_returning_fn(*args, **kwargs)\n",
    "                df.to_pickle(cache_key)\n",
    "                return df\n",
    "            \n",
    "        return get_with_cache\n",
    "    return cached_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_users(lang, start_date, end_date, min_rev_id):\n",
    "    \"\"\"\n",
    "    Rterun the first and last edits of only active users in `lang`wiki\n",
    "    between the start_date and end_date.\n",
    "    \"\"\"\n",
    "    con.execute(f'use {lang}wiki_p;')\n",
    "    active_sql = \"\"\"select distinct(rev_user) from revision \n",
    "    where {start_date} <= rev_timestamp and rev_timestamp <= {end_date}\n",
    "    and rev_id > {min_rev_id}\n",
    "    ;\n",
    "                \"\"\".format(start_date=to_wmftimestamp(start_date),\n",
    "           end_date=to_wmftimestamp(end_date),\n",
    "           lang=lang, min_rev_id=min_rev_id)\n",
    "    active_df = pd.read_sql(active_sql, con)\n",
    "    return active_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_cached_df('spans')\n",
    "def get_users_edit_spans(lang, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Return the the first and last edits of all users in `lang`wiki\n",
    "    between the start_date and end_date\n",
    "    \"\"\"\n",
    "    db_prefix = f'{lang}wiki_p'\n",
    "    con.execute(f'use {db_prefix};')\n",
    "    reg_sql = '''select '{lang}' as lang, user_id, user_name, user_registration,\n",
    "       (select min(rev_timestamp) from revision_userindex where rev_user=user_id and {start_date} <= rev_timestamp <= {end_date}) as first_edit, \n",
    "       (select max(rev_timestamp) from revision_userindex where rev_user=user_id and {start_date} <= rev_timestamp <= {end_date}) as last_edit\n",
    "from user where coalesce(user_registration, 20010101000000) <= {end_date} \n",
    "     and \n",
    "                coalesce(user_registration, 20010101000000) >= {start_date};\n",
    "'''.format(start_date=to_wmftimestamp(start_date),\n",
    "           end_date=to_wmftimestamp(end_date),\n",
    "           lang=lang)\n",
    "    span_df = pd.read_sql(reg_sql, con)\n",
    "    span_df['user_registration'] = span_df['user_registration'].apply(wmftimestamp)\n",
    "    span_df['first_edit'] = span_df['first_edit'].apply(wmftimestamp)\n",
    "    span_df['last_edit'] = span_df['last_edit'].apply(wmftimestamp)\n",
    "    span_df['user_name'] = span_df['user_name'].apply(decode_or_nan)\n",
    "    return span_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikipedia_start_date' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1fc578837b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmake_populations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwikipedia_start_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msim_treatment_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"for every registered user get first and last edit (or not of those users didn't edit in the period)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mspan_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlangs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mspan_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_users_edit_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wikipedia_start_date' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def make_populations(start_date=wikipedia_start_date, end_date=sim_treatment_date):\n",
    "    \"\"\"for every registered user get first and last edit (or not of those users didn't edit in the period)\"\"\"\n",
    "    span_dfs = []\n",
    "    for lang in langs:\n",
    "        span_df = get_users_edit_spans(lang, start_date, end_date)\n",
    "        span_dfs.append(span_df)\n",
    "    return pd.concat(span_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactive_users(df):\n",
    "    \"\"\"remove users who have no edits in the period\"\"\"\n",
    "    return df[(pd.notnull(df['first_edit'])) | (pd.notnull(df['last_edit']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_cached_df('thanks')\n",
    "def get_thanks_thanking_user(lang, user_name, start_date, end_date):\n",
    "    con.execute(f\"use {lang}wiki_p;\")\n",
    "    user_thank_sql = f\"\"\"\n",
    "                    select thank_timestamp, sender, receiver, ru.user_id as receiver_id, su.user_id as sender_id from\n",
    "                        (select log_timestamp as thank_timestamp, replace(log_title, '_', ' ') as receiver, log_user_text as sender\n",
    "                        from logging_logindex where log_title = '{user_name.replace(' ', '')}'\n",
    "                        and log_action = 'thank'\n",
    "                        and {to_wmftimestamp(start_date)} <= log_timestamp <= {to_wmftimestamp(end_date)}) t\n",
    "                    left join user ru on ru.user_name = t.receiver\n",
    "                    left join user su on su.user_name = t.sender \"\"\"\n",
    "    df = pd.read_sql(user_thank_sql, con)\n",
    "    df['thank_timestamp'] = df['thank_timestamp'].apply(wmftimestamp)\n",
    "    df['sender'] = df['sender'].apply(decode_or_nan)\n",
    "    df['receiver'] = df['receiver'].apply(decode_or_nan)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thanks(df, start_date, end_date, col_name):\n",
    "    user_thank_count_dfs = []\n",
    "    for lang in langs:\n",
    "        user_names =  user_names = df[df['lang']==lang]['user_name'].values\n",
    "        for user_name in user_names:\n",
    "            user_thank_df = get_thanks_thanking_user(lang, user_name, start_date, end_date)\n",
    "            user_thank_count_df = pd.DataFrame.from_dict({col_name:[len(user_thank_df)],\n",
    "                                           'user_name':[user_name], \n",
    "                                           'lang':[lang]}, orient='columns')\n",
    "            user_thank_count_dfs.append(user_thank_count_df)\n",
    "    \n",
    "    thank_counts_df = pd.concat(user_thank_count_dfs)\n",
    "    df = pd.merge(df, thank_counts_df, how='left', on=['lang', 'user_name'])\n",
    "    return df\n",
    "\n",
    "def add_thanks_pre_treatment(df):\n",
    "    return add_thanks(df, start_date=sim_observation_start_date, end_date=sim_treatment_date, col_name='num_thanks_received_pre_treatment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_from_td(delta):\n",
    "    bins_log2 = (0, 90, 180, 365, 730, 1460, 2920, 5840)\n",
    "    delta_days = delta.days\n",
    "    prev_threshold=0\n",
    "    for threshold in bins_log2:\n",
    "        if delta_days > threshold:\n",
    "            prev_threshold=threshold\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return f'bin_{prev_threshold}'\n",
    "\n",
    "\n",
    "def add_experience_bin(df):\n",
    "    df['days_since_registration'] = sim_treatment_date - df['user_registration']\n",
    "    df['experience_level_pre_treatment'] = df['days_since_registration'].apply(bin_from_td)\n",
    "    del df['days_since_registration']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_bin_stats(df):\n",
    "    bin_stats = pd.DataFrame(df.groupby(['experience_level_pre_treatment','lang']).size()).rename(columns={0:'users_with_at_least_days_experience'})\n",
    "    bin_stats.to_csv('outputs/bin_stats_df_one_edit_min.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting representative sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_lang_bin_groups = full_df.groupby(['active_in_90_pre_treatment', 'lang', 'experience_level_pre_treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sample_size = 200\n",
    "guess_needed_sample_size = 100 # some of the subsamples will not be valid because after testing for quality they won't have sufficient\n",
    "subsamples = []\n",
    "for (is_active, lang, bin_name), group in active_lang_bin_groups:\n",
    "    sample_size = guess_needed_sample_size\n",
    "    if bin_name == 'bin_0':\n",
    "        sample_size = sample_size * 3\n",
    "    if len(group) < sample_size:\n",
    "        sample_size = len(group)\n",
    "#     print(group.shape, sample_size)\n",
    "    subsample = group.sample(n=sample_size)\n",
    "    subsamples.append(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.concat(subsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['experience_level_pre_treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['active_in_90_pre_treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Revisions of Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_90_stroke_50_recent_edits(userid, lang):\n",
    "    '''this will get all the rev_ids for a user that occured less than 90 days before their last edit\n",
    "    and no more than 50 edits in total '''\n",
    "    con.execute('use {lang}wiki_p;'.format(lang=lang))\n",
    "    revsql = '''\n",
    "            select rev_user, ts, rev_id from\n",
    "            (select a.rev_user as rev_user, timestamp(a.rev_timestamp) as ts, a.rev_id as rev_id, timestamp(b.mts) as mts\n",
    "            from\n",
    "            (select rev_user, rev_timestamp, rev_id from revision_userindex where rev_user = {userid}) a\n",
    "            join\n",
    "            (select rev_user, max(rev_timestamp) as mts from revision_userindex where rev_user = {userid})  b\n",
    "            on a.rev_user = b.rev_user\n",
    "            ) uhist\n",
    "            where ts > date_sub(mts, interval 90 day)\n",
    "            limit 50;'''.format(userid=userid)\n",
    "    udf = pd.read_sql(revsql, con)\n",
    "    return udf\n",
    "\n",
    "def get_50_edits_before_sim_treatment(userid, lang):\n",
    "    '''this will get all the rev_ids and timestamps for a user that occured before the simulated treatment date and just the \n",
    "        50 most recent of those '''\n",
    "    con.execute('use {lang}wiki_p;'.format(lang=lang))\n",
    "    revsql = '''select rev_id, rev_timestamp, page_id, page_namespace from (select rev_id, rev_timestamp, rev_page from revision_userindex where rev_user = {userid}\n",
    "                and rev_timestamp < {datestr}\n",
    "                order by rev_timestamp desc\n",
    "                                            limit 50) revs\n",
    "                                            join page\n",
    "                                            on revs.rev_page = page.page_id;'''.format(datestr=sim_treatment_date.strftime('%Y%m%d%H%M%S'), userid=userid)\n",
    "    udf = pd.read_sql(revsql, con)\n",
    "    udf['rev_timestamp'] = udf['rev_timestamp'].apply(wmftimestamp)\n",
    "    return udf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-be1b8eddccbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revisions_before_sim_treatment_50'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_50_edits_before_sim_treatment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sub_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "sub_df['revisions_before_sim_treatment_50'] = sub_df.apply(lambda row: get_50_edits_before_sim_treatment(row['user_id'], row['lang']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['len_revs'] = sub_df['revisions_before_sim_treatment_50'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['len_revs'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df[sub_df['len_revs']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_pickle('checkpoints/sub_df_revs_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_pickle('checkpoints/sub_df_revs_1.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ores_string = 'https://ores.wikimedia.org/v3/scores/{context}/{revid}/{model}'\n",
    "\n",
    "def list_of_ores_scores(rev_ids, model, lang):\n",
    "#     print('lang', lang)\n",
    "    context = f'{lang}wiki'\n",
    "    ores_scores = []\n",
    "    ores_predicitons = []\n",
    "    for rev_id in rev_ids:\n",
    "        ores_req = ores_string.format(context=context, revid=rev_id, model=model)\n",
    "#         print(f'context: {context}')\n",
    "        # print(f'ores_req: {ores_req}')\n",
    "        ores_resp = requests.get(url=ores_req)\n",
    "        ores_data = ores_resp.json()\n",
    "#         print(f'ores_data: {ores_data}')\n",
    "        try:\n",
    "            ores_score = ores_data[context]['scores'][str(rev_id)][model]['score']['probability']['true']\n",
    "            ores_prediction = ores_data[context]['scores'][str(rev_id)][model]['score']['prediction']\n",
    "            ores_scores.append(ores_score)\n",
    "            ores_predicitons.append(ores_prediction)\n",
    "        except KeyError:\n",
    "            ores_scores.append(float('nan'))\n",
    "            ores_predicitons.append(float('nan'))\n",
    "    return {'ores_scores':ores_scores, 'ores_predictions':ores_predicitons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def get_from_ores_with_cache(rev_ids, model, lang):\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count, dt.now())\n",
    "    datadir = os.path.join('data', lang, model)\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "    fname = os.path.join(datadir,f'revlist_starting_{rev_ids[0]}')\n",
    "    if not os.path.exists(fname):\n",
    "        ores_data = list_of_ores_scores(rev_ids, model, lang)\n",
    "        json.dump(ores_data, open(fname, 'w'))\n",
    "        return ores_data\n",
    "    else:\n",
    "        return json.load(open(fname,'r'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_sub_df = sub_df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sub_sub_df['ores_damaging_data'] = sub_sub_df.apply(lambda row: get_from_ores_with_cache(\n",
    "#     rev_ids=[d['rev_id'] for d in row['revisions_before_sim_treatment_50']],\n",
    "#     model='damaging',\n",
    "#     lang=row['lang']),\n",
    "#     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "sub_df['ores_damaging_data'] = sub_df.apply(lambda row: get_from_ores_with_cache(\n",
    "    rev_ids=[d['rev_id'] for d in row['revisions_before_sim_treatment_50']],\n",
    "    model='damaging',\n",
    "    lang=row['lang']),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_pickle('checkpoints/sub_df_revs_ores_damaging_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_pickle('checkpoints/sub_df_revs_ores_damaging_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['num_non_damaging_pre_treatment'] = sub_df['ores_damaging_data'].apply(lambda d: len([pred for pred in d['ores_predictions'] if pred==False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "sub_df['ores_goodfaith_data'] = sub_df.apply(lambda row: get_from_ores_with_cache(\n",
    "    rev_ids=[d['rev_id'] for d in row['revisions_before_sim_treatment_50']],\n",
    "    model='goodfaith',\n",
    "    lang=row['lang']),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['num_goodfaith_pre_treatment'] = sub_df['ores_goodfaith_data'].apply(lambda d: len([pred for pred in d['ores_predictions'] if pred==True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_pickle('checkpoints/sub_df_revs_ores_damaging_goodfaith_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['num_goodfaith_pre_treatment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['num_non_damaging_pre_treatment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_pickle('checkpoints/sub_df_revs_ores_damaging_goodfaith_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_pickle('checkpoints/sub_df_with_de_without_rev_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data for batches for julia and co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we split into the german and non-german sets\n",
    "\n",
    "### for the non-german,\n",
    "+ we'll get the 50 most recent edits, \n",
    "+ and pull the ORES data for them\n",
    "+ and continue\n",
    "\n",
    "### for the german,\n",
    "+ use the flagged revisions method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_pickle('checkpoints/sub_df_with_de_without_rev_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_orig = sub_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_de = sub_df[sub_df['lang'] == 'de']\n",
    "\n",
    "sub_df = sub_df[sub_df['lang'] != 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only active users\n",
    "batch_eligble = sub_df[sub_df['active_in_90_pre_treatment']==True]\n",
    "batch_eligble_de = sub_df_de[sub_df_de['active_in_90_pre_treatment']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eligble.groupby('lang').agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eligble['newcomer_experienced'] = batch_eligble['experience_level_pre_treatment'].apply(lambda x: 'newcomer' if x=='bin_0' else 'experienced_90_days_plus')\n",
    "batch_eligble_de['newcomer_experienced'] = batch_eligble_de['experience_level_pre_treatment'].apply(lambda x: 'newcomer' if x=='bin_0' else 'experienced_90_days_plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eligble.groupby(by=['lang','newcomer_experienced']).agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eligble_de.groupby(by=['lang','newcomer_experienced']).agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batch_eligble.groupby(by=['lang','newcomer_experienced']).apply(lambda df: df.sample(n=200))\n",
    "batches_de = batch_eligble_de.groupby(by=['lang','newcomer_experienced']).apply(lambda df: df.sample(n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches.groupby('lang').agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batches['revisions_before_sim_treatment_50'] = batches.apply(lambda row: get_50_edits_before_sim_treatment(row['user_id'], row['lang']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "batches['ores_damaging_data'] = batches.apply(lambda row: get_from_ores_with_cache(\n",
    "    rev_ids=[d['rev_id'] for d in row['revisions_before_sim_treatment_50']],\n",
    "    model='damaging',\n",
    "    lang=row['lang']),\n",
    "    axis=1)\n",
    "count = 0\n",
    "batches['ores_goodfaith_data'] = batches.apply(lambda row: get_from_ores_with_cache(\n",
    "    rev_ids=[d['rev_id'] for d in row['revisions_before_sim_treatment_50']],\n",
    "    model='goodfaith',\n",
    "    lang=row['lang']),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we need to go to batches row-oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_17_edits_of_user(user_df):\n",
    "    batch_edits_dfs = []\n",
    "    rev_df = pd.DataFrame.from_dict(user_df['revisions_before_sim_treatment_50'].iloc[0])\n",
    "    ores_damaging_scores = user_df['ores_damaging_data'].apply(lambda d:d['ores_scores']).values[0]\n",
    "    ores_damaging_predictions = user_df['ores_damaging_data'].apply(lambda d:d['ores_predictions']).values[0]\n",
    "    ores_goodfaith_scores = user_df['ores_goodfaith_data'].apply(lambda d:d['ores_scores']).values[0]\n",
    "    ores_goodfaith_predictions = user_df['ores_goodfaith_data'].apply(lambda d:d['ores_predictions']).values[0]\n",
    "    try:\n",
    "        rev_df['ores_damaging_scores'] = ores_damaging_scores\n",
    "        rev_df['ores_damaging_predictions'] = ores_damaging_predictions\n",
    "        rev_df['ores_goodfaith_scores'] = ores_goodfaith_scores\n",
    "        rev_df['ores_goodfaith_predictions'] = ores_goodfaith_predictions\n",
    "    except ValueError: #occurs from some temporal mismatch where there are more ores data than revisions maybe some were deleted\n",
    "        return None #skip this user\n",
    "\n",
    "    rev_df = rev_df[rev_df['ores_damaging_predictions']==False]\n",
    "    rev_df = rev_df[rev_df['ores_goodfaith_predictions']==True]\n",
    "     \n",
    "    # even namespaces\n",
    "    # Limit to non-talk pages\n",
    "    rev_df = rev_df[rev_df['page_namespace'].apply(lambda ns: ns%2==0)]\n",
    "    # Limit to 17\n",
    "    rev_df = rev_df.iloc[:17]\n",
    "    \n",
    "    if len(rev_df)==0:\n",
    "        print('no nondamaging goodfaith for user: {}'.format(user_df['user_id'][0]))\n",
    "        return None\n",
    "    for row in rev_df.iterrows():\n",
    "        edit_df = user_df.copy()\n",
    "        edit_df['rev_id'] = row[1]['rev_id']\n",
    "        edit_df['rev_timestamp'] = row[1]['rev_timestamp']\n",
    "        edit_df['ores_damaging_score'] = row[1]['ores_damaging_scores']\n",
    "        edit_df['ores_damaging_prediction'] = row[1]['ores_damaging_predictions']\n",
    "        edit_df['ores_goodfaith_score'] = row[1]['ores_goodfaith_scores']\n",
    "        edit_df['ores_goodfaith_prediction'] = row[1]['ores_goodfaith_predictions']\n",
    "        edit_df['page_namespace'] = row[1]['page_namespace']\n",
    "\n",
    "        batch_edits_dfs.append(edit_df)\n",
    "    return pd.concat(batch_edits_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits = batches.groupby('user_id').apply(max_17_edits_of_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches_edits), (len(batches)*17)/2 + (len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits.to_pickle('checkpoints/batches_edits_non_de_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 173\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# compat pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 177\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 173\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=False))\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# compat pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e1bbcff8de26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches_edits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/batches_edits_non_de_1.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 177\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    144\u001b[0m         f, fh = _get_handle(path, 'rb',\n\u001b[1;32m    145\u001b[0m                             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             is_text=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/civilservant-wikipedia-gratitude-populatio-cg6eqquy/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/batches_edits_non_de_1.pickle'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "batches_edits = pd.read_pickle('checkpoints/batches_edits_non_de_1.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## german processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute('use dewiki_p;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "de_flagged_revs_users_d10 = [get_flagged_revs(user_id, sim_treatment_date) for user_id in batches_de.iloc[:10]['user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "de_flagged_revs_users = [get_flagged_revs(user_id, sim_treatment_date) for user_id in batches_de['user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de = pd.concat(de_flagged_revs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de.to_pickle('checkpoints/batches_edits_de_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de = pd.read_pickle('checkpoints/batches_edits_de_1.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finally put german back together and  need to sample down to 100 users per lang-binexperience group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de['lang']='de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_user_id_experience = batches_de['user_id'].reset_index()[['user_id','newcomer_experienced']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de['user_id'] = batches_edits_de['user_id'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de = batches_edits_de.merge(de_user_id_experience, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de.to_pickle('checkpoints/batches_edits_de_with_newcomer_experience1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_de = pd.read_pickle('checkpoints/batches_edits_de_with_newcomer_experience1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = ['lang', 'user_id','rev_id', 'newcomer_experienced', 'rev_timestamp']\n",
    "batches_edits_all = pd.concat([batches_edits[common_cols], batches_edits_de[common_cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches_edits), len(batches_edits_de), len(batches_edits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the first 100 of each lang/experience group\n",
    "be_user_ids = batches_edits_all.groupby(['lang','newcomer_experienced']).apply(lambda df: df['user_id'].unique()[:100]).reset_index().rename({0:'100_users'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lucky_100(user_id, lang, newcomer_experienced):\n",
    "    lucky_uids = be_user_ids[(be_user_ids['lang']==lang) & (be_user_ids['newcomer_experienced']==newcomer_experienced)]['100_users'].iloc[0]\n",
    "    return user_id in lucky_uids\n",
    "\n",
    "batches_edits_all['lucky_100'] = batches_edits_all.apply(lambda row: is_lucky_100(row['user_id'], row['lang'], row['newcomer_experienced']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample down\n",
    "batches_edits_100per = batches_edits_all[batches_edits_all['lucky_100'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per.groupby(['lang']).agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per.to_pickle('checkpoints/batches_edits_100per.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mwapi.errors import APIError\n",
    "\n",
    "cnt = 0\n",
    "def get_diff_html_dict(lang, revid):\n",
    "    global cnt\n",
    "    cnt += 1\n",
    "    if cnt%100==0:\n",
    "        print(cnt)\n",
    "    try:\n",
    "        mwapisession = mwapi.Session(host=f'https://{lang}.wikipedia.org', user_agent='civilservant datagathering <max@notconfusing.com>')\n",
    "        ret = mwapisession.get(action='compare', fromrev=revid, torelative='prev', prop='diff|user|parsedcomment|comment|size|rel|title')\n",
    "        ret_comp = ret['compare']\n",
    "    except APIError:\n",
    "        ret_comp = {'*':'Deleted revision','tosize':0,'toparsedcomment':'Deleted revision'}\n",
    "    #keys are: tosize, toparsedcomment, *\n",
    "    return ret_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "batches_edits_100per['diff_info'] = batches_edits_100per.apply(lambda row: get_diff_html_dict(row['lang'], row['rev_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per['html_blob'] = batches_edits_100per['diff_info'].apply(lambda d: d['*'])\n",
    "batches_edits_100per['diff_size'] = batches_edits_100per['diff_info'].apply(lambda d: d['tosize'])\n",
    "batches_edits_100per['edit_comment'] = batches_edits_100per['diff_info'].apply(lambda d: d['toparsedcomment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difflink(lang,rev_id):\n",
    "    basestr = 'https://{lang}.wikipedia.org/w/index.php?oldid={rev_id}'.format(lang=lang, rev_id=rev_id)\n",
    "    return basestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per['diff_link'] = batches_edits_100per.apply(lambda row: difflink(row['lang'], row['rev_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per.to_pickle('checkpoints/batches_edits_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_edits_100per = pd.read_pickle('checkpoints/batches_edits_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_save_columns = ['user_id', 'lang', 'newcomer_experienced',\n",
    "                   'rev_id', 'edit_comment',\n",
    "                   'diff_size', 'html_blob', 'diff_link',\n",
    "                   'rev_timestamp'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_for_out = batches_edits_100per[be_save_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_for_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_for_out.to_csv('outputs/user_edit_diffs_nondamaging_goodfaith_max17_evennamespaces_100_per_experiencelevel_per_lang_with_de_ts.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make labour sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps_90_before_after_sim_treatment(userid, lang, before_after_sim):\n",
    "    '''this will get all the timestamps of edits for a user that occured before or after 90 days before, upto the  simulated treatment date'''\n",
    "    if before_after_sim=='before':\n",
    "        start, end = sim_observation_start_date, sim_treatment_date\n",
    "    elif before_after_sim=='after':\n",
    "        start, end = sim_treatment_date, sim_experiment_end_date\n",
    "        \n",
    "    con.execute('use {lang}wiki_p;'.format(lang=lang))\n",
    "    revsql = '''select rev_timestamp from revision_userindex where rev_user = {userid}\n",
    "                and rev_timestamp >= {datestr_start} and rev_timestamp < {datestr_end} \n",
    "                order by rev_timestamp\n",
    "                '''.format(datestr_end=end.strftime('%Y%m%d%H%M%S'), \n",
    "                           datestr_start=start.strftime('%Y%m%d%H%M%S'),\n",
    "                           userid=userid)\n",
    "    udf = pd.read_sql(revsql, con)\n",
    "    udf['rev_timestamp'] = udf['rev_timestamp'].apply(wmftimestamp)\n",
    "    return list(udf['rev_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['timestamps_90_before_sim_treatment'] = sub_df.apply(lambda row: get_timestamps_90_before_after_sim_treatment(row['user_id'], row['lang'], 'before'), axis=1) \n",
    "sub_df['timestamps_90_after_sim_treatment'] = sub_df.apply(lambda row: get_timestamps_90_before_after_sim_treatment(row['user_id'], row['lang'], 'after'), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more german power data finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_de = pd.read_pickle('checkpoints/sub_df_de_with_thanks.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_de['timestamps_90_before_sim_treatment'] = sub_df_de.apply(lambda row: get_timestamps_90_before_after_sim_treatment(row['user_id'], row['lang'], 'before'), axis=1) \n",
    "sub_df_de['timestamps_90_after_sim_treatment'] = sub_df_de.apply(lambda row: get_timestamps_90_before_after_sim_treatment(row['user_id'], row['lang'], 'after'), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_count = 0 \n",
    "def num_flagged_in_last_50(user_id, sim_treatment_date):\n",
    "    global flagged_count\n",
    "    if flagged_count % 100 == 0:\n",
    "        print(flagged_count)\n",
    "    flagged_count += 1\n",
    "    deuser_pickle_fname = f'data/deusers/{user_id}.pickle'\n",
    "    if not os.path.exists(deuser_pickle_fname):\n",
    "        deuser = get_flagged_revs(user_id, sim_treatment_date, return_all=True)\n",
    "        deuser.to_pickle(deuser_pickle_fname)\n",
    "    else:\n",
    "        deuser = pd.read_pickle(deuser_pickle_fname)    \n",
    "    \n",
    "    num_considerations = 50\n",
    "    deuser_short = deuser.sort_values('rev_timestamp', ascending=False).iloc[:num_considerations]\n",
    "    num_flagged = len(deuser_short[deuser_short['flagged']==True])\n",
    "    return num_flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_de['num_flagged_revisions_pre_treatment'] = sub_df_de['user_id'].apply(lambda u: num_flagged_in_last_50(u, sim_treatment_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sessions(ts_list):\n",
    "    #these structures store the timestamps\n",
    "    edit_sessions = []\n",
    "    curr_edit_session = []\n",
    "\n",
    "    #initialize prev to the earliest data possible\n",
    "    prev_timestamp = datetime.datetime(year=2001, month=1, day=1)\n",
    "\n",
    "    for index, ts in enumerate(ts_list):\n",
    "#         print('index:', index)\n",
    "        curr_timestamp = ts\n",
    "        #if curr timestamp within 1 hour of last then append\n",
    "        if curr_timestamp - prev_timestamp < datetime.timedelta(hours=1):\n",
    "            curr_edit_session.append(curr_timestamp)\n",
    "        # else start a new edit session\n",
    "        else:\n",
    "            #if there's a pre-existing session save it to the return\n",
    "            if curr_edit_session:\n",
    "                edit_sessions.append(curr_edit_session)\n",
    "            # and start a new session\n",
    "            curr_edit_session = [curr_timestamp]\n",
    "        # this is before\n",
    "        if index < len(ts_list)-1:\n",
    "            prev_timestamp = curr_timestamp\n",
    "        # this is the last item save this session too.\n",
    "        else:\n",
    "#             print('this is the end')\n",
    "            edit_sessions.append(curr_edit_session)\n",
    "        \n",
    "    return edit_sessions\n",
    "\n",
    "def labour_hours(ts_list):\n",
    "    sessions = make_sessions(ts_list)\n",
    "    total_labour_hours = 0\n",
    "    for session in sessions:\n",
    "        if len(session) == 1:\n",
    "            total_labour_hours += 1\n",
    "        else:\n",
    "            session_duration = max(session) - min(session)\n",
    "            session_seconds = session_duration.seconds\n",
    "            session_hours = session_seconds / (60*60)\n",
    "            session_hours += 1 # for this session\n",
    "            total_labour_hours += session_hours\n",
    "    return total_labour_hours\n",
    "\n",
    "def ts_in_week(ts_list, date_start, date_end):\n",
    "    in_week = []\n",
    "    for ts in ts_list:\n",
    "        if ts > date_start and ts <= date_end:\n",
    "            in_week.append(ts)\n",
    "    return in_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_de['labour_hours_90_pre_treatment'] = sub_df_de['timestamps_90_before_sim_treatment'].apply(labour_hours)\n",
    "\n",
    "sub_df_de['num_edits_90_pre_treatment'] = sub_df_de['timestamps_90_before_sim_treatment'].apply(len)\n",
    "\n",
    "sub_df_de['labour_hours_90_post_treatment'] = sub_df_de['timestamps_90_after_sim_treatment'].apply(labour_hours)\n",
    "\n",
    "sub_df_de['num_edits_90_post_treatment'] = sub_df_de['timestamps_90_after_sim_treatment'].apply(len)\n",
    "\n",
    "sub_df_de[['num_edits_90_pre_treatment', 'num_edits_90_post_treatment']].plot()\n",
    "\n",
    "sub_df_de[['labour_hours_90_pre_treatment', 'labour_hours_90_post_treatment']].plot()\n",
    "\n",
    "sub_df_de.to_pickle('checkpoints/sub_df_de_revs_ts_labour_hours_1.pickle')\n",
    "\n",
    "sub_df_de = pd.read_pickle('checkpoints/sub_df_de_revs_ts_labour_hours_1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['labour_hours_90_pre_treatment'] = sub_df['timestamps_90_before_sim_treatment'].apply(labour_hours)\n",
    "\n",
    "sub_df['num_edits_90_pre_treatment'] = sub_df['timestamps_90_before_sim_treatment'].apply(len)\n",
    "\n",
    "sub_df['labour_hours_90_post_treatment'] = sub_df['timestamps_90_after_sim_treatment'].apply(labour_hours)\n",
    "\n",
    "sub_df['num_edits_90_post_treatment'] = sub_df['timestamps_90_after_sim_treatment'].apply(len)\n",
    "\n",
    "sub_df[['num_edits_90_pre_treatment', 'num_edits_90_post_treatment']].plot()\n",
    "\n",
    "sub_df[['labour_hours_90_pre_treatment', 'labour_hours_90_post_treatment']].plot()\n",
    "\n",
    "sub_df.to_pickle('checkpoints/sub_df_revs_ts_labour_hours_1.pickle')\n",
    "\n",
    "sub_df = pd.read_pickle('checkpoints/sub_df_revs_ts_labour_hours_1.pickle')\n",
    "\n",
    "inactive = sub_df[sub_df['active_in_90_pre_treatment']==False]\n",
    "active = sub_df[sub_df['active_in_90_pre_treatment']==True]\n",
    "\n",
    "active['num_edits_90_pre_treatment'].mean()\n",
    "\n",
    "active['num_edits_90_post_treatment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_col_f = 'num_edits_week_{ind}_post_treatment'\n",
    "anye_col_f = 'any_edits_week_{ind}_post_treatment'\n",
    "labh_col_f = 'labour_hours_week_{ind}_post_treatment'\n",
    "anyl_col_f = 'any_labour_hours_week_{ind}_post_treatment'\n",
    "\n",
    "col_fn ={edit_col_f: len,\n",
    "         anye_col_f: lambda x: len(x)>0,\n",
    "         labh_col_f: lambda x: labour_hours(x),\n",
    "         anyl_col_f: lambda x: labour_hours(x)>0}\n",
    "\n",
    "sub_df_list = []\n",
    "try:\n",
    "    sub_df_list.append(sub_df)\n",
    "except NameError:\n",
    "    pass\n",
    "try:\n",
    "    sub_df_list.append(sub_df_de)\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "for col_f, fn in col_fn.items():\n",
    "    for ind, (week_n_start, week_n_end) in enumerate(window_seq(range(13))):\n",
    "        ind = ind+1\n",
    "        days_after_treat_start = week_n_start * 7 \n",
    "        days_after_treat_end = week_n_end * 7\n",
    "        date_start = sim_treatment_date + td(days=days_after_treat_start)\n",
    "        date_end = sim_treatment_date + td(days=days_after_treat_end)\n",
    "\n",
    "        col = col_f.format(ind=ind)\n",
    "        print(col)\n",
    "\n",
    "        ts_week = f'ts_in_week_{ind}'\n",
    "        \n",
    "        for sub_df in sub_df_list:\n",
    "            sub_df[ts_week] = sub_df['timestamps_90_after_sim_treatment'].apply(lambda x: ts_in_week(x, date_start, date_end))\n",
    "            sub_df[col] = sub_df[ts_week].apply(fn)\n",
    "\n",
    "\n",
    "            del sub_df[ts_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_cached_df('disablemail')\n",
    "def get_user_disablemail_properties(lang, user_id):\n",
    "    con.execute(f\"use {lang}wiki_p;\")\n",
    "    user_prop_sql = f\"\"\"select * from user_properties where up_user = {user_id}\n",
    "                        and up_property = 'disablemail';\"\"\"\n",
    "    df = pd.read_sql(user_prop_sql, con)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_has_email_currently(df):\n",
    "    user_prop_dfs = []\n",
    "    for lang in langs:\n",
    "        user_ids =  user_ids = df[df['lang']==lang]['user_id'].values\n",
    "        for user_id in user_ids:\n",
    "            user_prop_df = get_user_disablemail_properties(lang, user_id)\n",
    "            has_email = False if len(user_prop_df) >=1 else True #the property disables email, if it doesn't exist the default its that it's on\n",
    "            user_prop_dfs.append(pd.DataFrame.from_dict({'has_email':[has_email],\n",
    "                                           'user_id':[user_id], \n",
    "                                           'lang':[lang]}, orient='columns'))\n",
    "    \n",
    "    users_prop_df = pd.concat(user_prop_dfs)\n",
    "    df = pd.merge(df, users_prop_df, how='left', on=['lang', 'user_id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_cached_df('total_edits')\n",
    "def get_total_user_edits(lang, user_id, start_date, end_date):\n",
    "    con.execute(f\"use {lang}wiki_p;\")\n",
    "    user_edit_sql = f\"\"\"select count(*) as edits_pre_treatment from revision_userindex \n",
    "                where rev_user = {user_id} \n",
    "                and {to_wmftimestamp(start_date)} <= rev_timestamp <= {to_wmftimestamp(end_date)};\n",
    "                \"\"\"\n",
    "    df = pd.read_sql(user_edit_sql, con)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_total_edits(df, start_date, end_date):\n",
    "    user_edit_dfs = []\n",
    "    for lang in langs:\n",
    "        user_ids =  user_ids = df[df['lang']==lang]['user_id'].values\n",
    "        for user_id in user_ids:\n",
    "            user_edit_df = get_total_user_edits(lang, user_id, start_date, end_date)\n",
    "            user_edit_df['user_id'] = user_id\n",
    "            user_edit_df['lang'] = lang\n",
    "            user_edit_dfs.append(user_edit_df)\n",
    "    \n",
    "    users_edit_df = pd.concat(user_edit_dfs)\n",
    "    df = pd.merge(df, users_edit_df, how='left', on=['lang', 'user_id'])\n",
    "    return df\n",
    "\n",
    "def add_total_edits_pre_treatment(df):\n",
    "    return add_total_edits(df, start_date=wikipedia_start_date, end_date=sim_treatment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to make data\n",
      "subsetting to 100 samples\n"
     ]
    }
   ],
   "source": [
    "def make_data(subsample=None):\n",
    "    print('starting to make data')\n",
    "    df = make_populations()\n",
    "    df = remove_inactive_users(df)\n",
    "    if subsample:\n",
    "        print(f'subsetting to {subsample} samples')\n",
    "        df = df.sample(n=subsample, random_state=1854)\n",
    "    \n",
    "    df = add_thanks_pre_treatment(df)\n",
    "    df = add_experience_bin(df)\n",
    "    if not subsample:\n",
    "        output_bin_stats(df)\n",
    "    \n",
    "    df = add_total_edits_pre_treatment(df)\n",
    "    df = add_has_email_currently(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subsample = os.getenv('subsample', 100)\n",
    "    df = make_data(subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_edits(user_df):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_registration</th>\n",
       "      <th>first_edit</th>\n",
       "      <th>last_edit</th>\n",
       "      <th>num_thanks_received_pre_treatment</th>\n",
       "      <th>experience_level_pre_treatment</th>\n",
       "      <th>edits_pre_treatment</th>\n",
       "      <th>has_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de</td>\n",
       "      <td>2844726</td>\n",
       "      <td>Tvaktuell</td>\n",
       "      <td>2018-02-01 15:08:52</td>\n",
       "      <td>2018-02-01 15:29:22</td>\n",
       "      <td>2018-02-01 16:02:31</td>\n",
       "      <td>0</td>\n",
       "      <td>bin_0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>1108371</td>\n",
       "      <td>Sohaib el hidouri</td>\n",
       "      <td>2015-12-08 20:57:18</td>\n",
       "      <td>2015-12-21 13:18:17</td>\n",
       "      <td>2016-01-14 21:02:47</td>\n",
       "      <td>0</td>\n",
       "      <td>bin_730</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de</td>\n",
       "      <td>2748865</td>\n",
       "      <td>Clarundius</td>\n",
       "      <td>2017-10-05 10:24:33</td>\n",
       "      <td>2017-10-12 13:36:35</td>\n",
       "      <td>2017-10-13 12:35:21</td>\n",
       "      <td>0</td>\n",
       "      <td>bin_90</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pl</td>\n",
       "      <td>867217</td>\n",
       "      <td>Jinowolski</td>\n",
       "      <td>2017-12-29 23:13:57</td>\n",
       "      <td>2017-12-29 23:19:16</td>\n",
       "      <td>2017-12-29 23:19:16</td>\n",
       "      <td>0</td>\n",
       "      <td>bin_0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>2805566</td>\n",
       "      <td>Jdfisher</td>\n",
       "      <td>2017-12-13 08:13:31</td>\n",
       "      <td>2006-11-27 17:16:11</td>\n",
       "      <td>2009-06-27 15:14:41</td>\n",
       "      <td>0</td>\n",
       "      <td>bin_0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  user_id          user_name   user_registration          first_edit  \\\n",
       "0   de  2844726          Tvaktuell 2018-02-01 15:08:52 2018-02-01 15:29:22   \n",
       "1   ar  1108371  Sohaib el hidouri 2015-12-08 20:57:18 2015-12-21 13:18:17   \n",
       "2   de  2748865         Clarundius 2017-10-05 10:24:33 2017-10-12 13:36:35   \n",
       "3   pl   867217         Jinowolski 2017-12-29 23:13:57 2017-12-29 23:19:16   \n",
       "4   de  2805566           Jdfisher 2017-12-13 08:13:31 2006-11-27 17:16:11   \n",
       "\n",
       "            last_edit  num_thanks_received_pre_treatment  \\\n",
       "0 2018-02-01 16:02:31                                  0   \n",
       "1 2016-01-14 21:02:47                                  0   \n",
       "2 2017-10-13 12:35:21                                  0   \n",
       "3 2017-12-29 23:19:16                                  0   \n",
       "4 2009-06-27 15:14:41                                  0   \n",
       "\n",
       "  experience_level_pre_treatment  edits_pre_treatment  has_email  \n",
       "0                          bin_0                    6          0  \n",
       "1                        bin_730                   30          0  \n",
       "2                         bin_90                   25          0  \n",
       "3                          bin_0                    1          0  \n",
       "4                          bin_0                    4          0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['has_email'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_minimum_rev_id(lang, start_date):\n",
    "    con.execute(f'use {lang}wiki_p;')\n",
    "    min_sql = f\"\"\"select * from revision where rev_timestamp <= {to_wmftimestamp(start_date)} order by rev_timestamp desc limit 1;\"\"\"\n",
    "    min_df = pd.read_sql(min_sql, con)\n",
    "    return min_df['rev_id'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rev_ids = {}\n",
    "for lang in langs:\n",
    "    min_rev_ids[lang] = get_a_minimum_rev_id(lang, sim_observation_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': 25877148, 'fa': 21737205, 'pl': 51165195, 'de': 171711383}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_rev_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  add has email to another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_dir = '/home/paprika/Tresors/CivilServant/projects/wikipedia-integration/gratitude-study/datasets/power_analysis/'\n",
    "fa_ar_pl_csv = 'gratitude_power-analysis_dataset_sim_date_20180306_v1.csv'\n",
    "de_csv = 'de_gratitude_power-analysis_dataset_sim_date_20180306_v1.csv'\n",
    "thankees = pd.read_csv(os.path.join(pa_dir, fa_ar_pl_csv), index_col=0)\n",
    "thankees_de = pd.read_csv(os.path.join(pa_dir, de_csv), index_col=0)\n",
    "\n",
    "thankees = add_has_email_currently(thankees)\n",
    "\n",
    "thankees_de = add_has_email_currently(thankees_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     8416\n",
       "False     171\n",
       "Name: has_email, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thankees['has_email'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1759\n",
       "False      41\n",
       "Name: has_email, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thankees_de['has_email'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "thankees.to_csv(os.path.join(pa_dir, fa_ar_pl_csv.split('v1.csv')[0]+\"with_email.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "thankees_de.to_csv(os.path.join(pa_dir, de_csv.split('v1.csv')[0]+\"with_email.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
